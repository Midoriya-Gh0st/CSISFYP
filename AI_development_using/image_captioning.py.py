# -*- coding: utf-8 -*-
"""CS4618-FYP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1648Ou9vpSuINUJz3KoCT3djG-3O_pbn7

## [1]. Importing the relative modules
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import collections
import random
import re
import numpy as np
import os
import time
import json
import pickle

from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from glob import glob
from PIL import Image

"""## [2]. Dataset download & preprocess

#### Download dataset
"""

# [√] - files 

# Download caption annotation files
annotation_folder = '/annotations/'
if not os.path.exists(os.path.abspath('.') + annotation_folder):
  annotation_zip = tf.keras.utils.get_file('captions.zip', cache_subdir=os.path.abspath('.'),
    origin='http://images.cocodataset.org/annotations/annotations_trainval2014.zip', extract=True)
  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'
  os.remove(annotation_zip)

# Download image files
image_folder = '/train2014/'
if not os.path.exists(os.path.abspath('.') + image_folder):
  image_zip = tf.keras.utils.get_file('train2014.zip', cache_subdir=os.path.abspath('.'),
    origin='http://images.cocodataset.org/zips/train2014.zip', extract=True)
  PATH = os.path.dirname(image_zip) + image_folder
  os.remove(image_zip)
else:
  PATH = os.path.abspath('.') + image_folder

# [√]
# Using with open can avoid cumbersome try...finally, 
# and will automatically call f.close().'r' means open in read-only mode
with open(annotation_file, 'r') as f:
    annotations = json.load(f)
# annotation is of dict type

"""#### Combine captions with the same image ID"""

# [√]

# Create a dictionary dict corresponding to image and caption
# i.e.: When the key in the dictionary does not exist but is searched, the returned value is not a keyError but a default value
image_path_to_caption = collections.defaultdict(list)
# at this time, the return value of image_caption_dict[1] should be a'[]', len(dict)=0

print(len(image_path_to_caption))
print(image_path_to_caption[1])
print(image_path_to_caption[10])

# [√]
# Important operation -> Combine captions with the same photo ID

# Remember to clear the array list

i = 0
for val in annotations['annotations']:
  # literal format string
  caption = f"<start> {val['caption']} <end>"  # Directly pass in the caption field in json
  
  # Take the ['image_id'] field from json and send it to the previous string template. 
  # The reason for using% here is to unify the number of digits to 12.
  image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])

  # print(caption)
  # print(image_path)
  # print(image_path_to_caption[image_path])

  image_path_to_caption[image_path].append(caption)
  # print(image_path_to_caption[image_path])

  if i % 10 == 0:
    print(">>> Index: ", i, end='\r')
  i += 1
  # print("------------")

print(">>> I:", len(annotations['annotations']))
print(len(image_path_to_caption))

"""#### Store image path as list"""

# [√]
# Store image path as list
image_paths = list(image_path_to_caption.keys())
random.shuffle(image_paths)

train_image_paths = image_paths[:30000] # No interception is required in the formal operation / save time at trial stage
print(len(train_image_paths))

len(image_paths)

# [√]

# fullfil all the captions & img_path
train_captions = []  # put all the captions
img_name_vector = []

i = 0
for image_path in train_image_paths:
  # One image corresponds to multiple captions, where caption_list corresponds to one image
  caption_list = image_path_to_caption[image_path] 
  train_captions.extend(caption_list)
  # Here represents the [image_path] name of the number of len(caption_list)
  img_name_vector.extend([image_path] * len(caption_list))

  # print(len(image_path_to_caption[image_path]))
  # print(caption_list) 
  # output the caption of each img, the number in front means there are several captions, most of them are 5

  if i % 10 == 0:
    print(">>> Index: ", i, end='\r')
  i += 1

# Check
print(train_captions[0])
print(train_captions[1])
print(train_captions[2])
print(img_name_vector[0])
Image.open(img_name_vector[0])

"""## [3]. Preprocess the images using Inception-V3

#### Get the Interception-V3 model
"""

# [√]

# image load
def load_image(image_path):
    img = tf.io.read_file(image_path)  # convert to string type tensor
    img = tf.image.decode_jpeg(img, channels=3)  # decode the picture into unit8 tensor
    img = tf.image.resize(img, (299, 299))  # resizing - img should meet the size specified by inception-v3
    img = tf.keras.applications.inception_v3.preprocess_input(img)  # normalize the image so that it contains pixels in [-1, 1]  

    return img, image_path

# Build inception-v3 model
# The shape of the last convolutional layer of iv3 is (8*8*2048)
# Feed each picture into the network and store the result vector in the dictionary (image_name --> feature_vector)

image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')
new_input = image_model.input  # Get the input of the model-return a tensor
hidden_layer = image_model.layers[-1].output # hidden_layer is the output of the last layer of iv3

# build a model based on input and output
# used to extract image features
image_features_extract_model = tf.keras.Model(inputs=new_input, outputs=hidden_layer)

print(new_input)
print(hidden_layer)

"""#### Pre-process each image with InceptionV3 and cache the output to disk."""

# Get unique images
encode_train = sorted(set(img_name_vector)) # len=1000

image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)
image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16) # apply load_image()

i = 0
for img, path in image_dataset: # len=63 (1000/16 ~=)
  # print(path)
  # use custom model to process img to get features
  batch_features = image_features_extract_model(img) 
  batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))
  # print(batch_features.shape) # 16, 64, 2048 (16 is because the batch is 16, that is, a data item has 16 pictures)

  for bf, p in zip(batch_features, path): # decode and store each data in the data item of size 16 once
    # print(p)
    path_of_feature = p.numpy().decode("utf-8")
    # print(path_of_feature, bf)
    np.save(path_of_feature, bf.numpy()) # use .numpy, just take the matrix

  if i % 1 == 0:
    print(">>> Index: ", i, end='\r')
  i += 1

path_of_feature

"""## [4]. Tokenizer establishment (Captions content processing)"""

top_k = 5000
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,
        oov_token="<unk>",
        filters='!"#$%&()*+.,-/:;=?@[\]^_`{|}~ ') # special char

tokenizer.fit_on_texts(train_captions)

config = tokenizer.get_config()

print(config)

# [√]

# pad stands for padding characters, that is, the characters used when filling the length later.
tokenizer.word_index['<pad>'] = 0
tokenizer.index_word[0] = '<pad>'

print(tokenizer.word_index["<start>"])
print(tokenizer.word_index["<end>"])
print(tokenizer.word_index["<unk>"])
print(tokenizer.word_index['<pad>'])
print(tokenizer.index_word[2])

# save the tokenizer  
joblib.dump(tokenizer, 'data_tokenizer.pkl')  # tok model save
# tokenizer = joblib.load('dataFile.pkl') # tok model load

# [√]
# Create the tokenized vectors, transforms each text in texts to a sequence of integers.
train_seqs = tokenizer.texts_to_sequences(train_captions)

# Pad each vector to the max_length of the captions
cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post') # pad from the end

print(train_seqs)
print(cap_vector)

# [√]

# Calculates the max_length, which is used to store the attention weights
# Find the maximum length of any caption in the dataset
def calc_max_length(tensor):
    return max(len(t) for t in tensor)

max_length = calc_max_length(train_seqs)

max_length

"""## [5]. Split the data into training and testing"""

# [√]

# make each img vector get its own multiple captions-img is the key, and one key has multiple values
img_to_cap_vector = collections.defaultdict(list)
for img, cap in zip(img_name_vector, cap_vector):
  img_to_cap_vector[img].append(cap)

img_keys = list(img_to_cap_vector.keys())
random.shuffle(img_keys)

slice_index = int(len(img_keys)*0.8) # 80% for training
img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]

# [√]

# get train/val dataset by the key above
img_name_train = []
cap_train = []
for imgt in img_name_train_keys:
  capt_len = len(img_to_cap_vector[imgt]) # get the number of capts of this image
  img_name_train.extend([imgt] * capt_len) # use the extension extend to add the same name of capt_len imgt
  cap_train.extend(img_to_cap_vector[imgt]) # put in imgt

# same as above
img_name_val = []
cap_val = []
for imgv in img_name_val_keys:
  capv_len = len(img_to_cap_vector[imgv])
  img_name_val.extend([imgv] * capv_len)
  cap_val.extend(img_to_cap_vector[imgv])

len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)

img_name_train[0]

"""## [6]. Create a tf.data dataset for training

"""

# [√]

# set the training hyperparameter 
BATCH_SIZE = 64 
BUFFER_SIZE = 1000  
embedding_dim = 256
units = 512
vocab_size = top_k + 1
num_steps = len(img_name_train) // BATCH_SIZE # (?)
# Shape of the vector extracted from Inception-V3 is (64, 2048)
# These two variables represent that vector shape
features_shape = 2048
attention_features_shape = 64

# self-defined func
# Load the numpy files
def map_func(img_name, cap):
  img_tensor = np.load(img_name.decode('utf-8')+'.npy') 

  return img_tensor, cap

dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))

# buffer_size:
# For prefetch: represents the maximum number of elements that will be added to the buffer. 
# The buffer_size in Dataset.prefetch() only affects the time to generate the next element.
# -> represents the maximum number of buffer elements during prefetching. 
# Set to AUTOTUNE, then the buffer size will be dynamically adjusted
# For shuffle: represents the number of elements from the dataset, from which the new dataset will be sampled. 
# The buffer_size in Dataset.shuffle() will affect the randomness of your dataset, that is, the order in which the elements are generated.

# Use map to load the numpy files in parallel
dataset = dataset.map(lambda item1, item2: tf.numpy_function(
          map_func, inp=[item1, item2], Tout=[tf.float32, tf.int32]), # map_func self-defined
          num_parallel_calls=tf.data.AUTOTUNE) # num_parallel_calls so that map uses multiple threads to process elements

# Shuffle and batch
dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)  # each data item thrown by the buffer contains batch_size data-to prevent data from overfitting
dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)  # creates a Dataset that prefetches elements from this dataset.
# This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.

dataset

"""## [7]. Model

#### Attention
"""

class BahdanauAttention(tf.keras.Model):
    def __init__(self, units):
        super(BahdanauAttention, self).__init__()  # will use 3 dense models
        self.W1 = tf.keras.layers.Dense(units)
        self.W2 = tf.keras.layers.Dense(units)
        self.V = tf.keras.layers.Dense(1)

    def call(self, features, hidden):
        hidden_with_time_axis = tf.expand_dims(hidden, 1)  # must have the same number of digits as features (a, b, c)

        # - calculating the alignment vector (score)
        # - calculated between the previous decoder hidden state and each of the encoder’s hidden states
        # - measure the similarity between the environment vector and the current input vector; 
        # - find out which input information should be focused in the current environment;
        # - features: input vector. hwta: environment vector
        # - each value of the alignment vector is the score (or probability) of the corresponding word in the original sequence
        # - attention_hidden_layer = (tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))) # same shape

        # - score shape == (batch_size, 64, 1) 
        # - a forward propagation network FNN, calculated
        # - score = self.V(attention_hidden_layer)
        score = self.V((tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))))  # score=eij, ( hj, st-1 )

        # - attention_weights shape == (batch_size, 64, 1)
        # - alignment function：compute attention weight，commonly use softmax to normalise.
        attention_weights = tf.nn.softmax(score, axis=1)  # atj

        # - context_vector shape after sum == (batch_size, hidden_size)
        context_vector = attention_weights * features
        context_vector = tf.reduce_sum(context_vector, axis=1)  # sum(atj*hj)

        return context_vector, attention_weights

"""#### CNN-RNN"""

class CNN_Encoder(tf.keras.Model):
  def __init__(self, embedding_dim):
    super(CNN_Encoder, self).__init__()
    self.fc = tf.keras.layers.Dense(embedding_dim)
    # shape after fc == (batch_size, 64, embedding_dim)

  def call(self, x):
    x = self.fc(x)
    x = tf.nn.relu(x)
    return x

class RNN_Decoder(tf.keras.Model):
    def __init__(self, embedding_dim, units, vocab_size):
        super(RNN_Decoder, self).__init__()
        self.units = units

        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.gru = tf.keras.layers.GRU(self.units,
                    return_sequences=True,
                    return_state=True,
                    recurrent_initializer='glorot_uniform')
        self.fc1 = tf.keras.layers.Dense(self.units)
        self.fc2 = tf.keras.layers.Dense(vocab_size)  # number of actual word in vocabulary

        self.attention = BahdanauAttention(self.units)

    @tf.function
    def call(self, x, features, hidden):
        # defining attention as a separate model
        context_vector, attention_weights = self.attention(features, hidden)

        # x shape after passing through embedding == (batch_size, 1, embedding_dim)
        # - x is the input, start to word embedding
        x = self.embedding(x)

        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)
        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)

        # passing the concatenated vector to the GRU
        output, state = self.gru(x)

        # shape == (batch_size, max_length, hidden_size)
        x = self.fc1(output)

        # x shape == (batch_size * max_length, hidden_size)
        # - dimensiona reduction
        x = tf.reshape(x, (-1, x.shape[2]))

        # output shape == (batch_size * max_length, vocab)
        x = self.fc2(x)

        return x, state, attention_weights

    def reset_state(self, batch_size):
        return tf.zeros((batch_size, self.units))

    def get_config(self):
        return {"features": self.features, "hidden": self.hidden}

encoder = CNN_Encoder(embedding_dim)
decoder = RNN_Decoder(embedding_dim, units, vocab_size)

# tokenizer model save
joblib.dump(encoder, 'obj_encoder.pkl')
joblib.dump(decoder, 'obj_decoder.pkl')

optimizer = tf.keras.optimizers.Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none') # cross entropy loss function
# return Weighted loss float Tensor

def loss_function(real, pred):
  mask = tf.math.logical_not(tf.math.equal(real, 0)) # output True/False
  loss_ = loss_object(real, pred) # y_true, y_pred

  mask = tf.cast(mask, dtype=loss_.dtype)
  loss_ *= mask # for padding 

  return tf.reduce_mean(loss_)

"""## [8]. Checkpoint"""

checkpoint_path = "./checkpoints/train"
ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)
ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)

start_epoch = 0
if ckpt_manager.latest_checkpoint:
  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])
  # restoring the latest checkpoint in checkpoint_path
  ckpt.restore(ckpt_manager.latest_checkpoint)

"""## [9]. Training 

"""

# adding this in a separate cell because if you run the training cell
# many times, the loss_plot array will be reset
loss_plot = []

@tf.function
def train_step(img_tensor, target):
  loss = 0

  # initializing the hidden state for each batch
  # because the captions are not related from image to image
  hidden = decoder.reset_state(batch_size=target.shape[0])

  print('target:', target)
  print('target-shape:', target.shape) # 64
  # -> [3, 3, 3, 3, 3, 3]
  print("test: ", [tokenizer.word_index['<start>']], target.shape[0], [tokenizer.word_index['<start>']] * target.shape[0])
  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)
  print('dec_input:', dec_input)

  with tf.GradientTape() as tape:
      features = encoder(img_tensor) # features extract

      # -Is target a caption? # target.shape[1] is the length of the caption (set to the maximum length)
      for i in range(1, target.shape[1]): 
          # passing the features through the decoder
          predictions, hidden, _ = decoder(dec_input, features, hidden) # get the predicted captions

          loss += loss_function(target[:, i], predictions) # judge the loss of target caption and yuce caption

          # using teacher forcing
          dec_input = tf.expand_dims(target[:, i], 1) # ------- decoder_input, take the first i words as input

  total_loss = (loss / int(target.shape[1])) # the sum of the loss of the sentence, excluding the sentence length

  trainable_variables = encoder.trainable_variables + decoder.trainable_variables # set trainable variables parameters

  gradients = tape.gradient(loss, trainable_variables) # Gradient calculation, training data includes encoder and decoder

  optimizer.apply_gradients(zip(gradients, trainable_variables)) # optimization

  return loss, total_loss

EPOCHS = 20

# target: [64, 34], 64, length 20.
# 64 is there a single image with 64, or 64 images?\
# -> 64 images, each of the captions first use [3] that is <start> placeholder

for epoch in range(start_epoch, EPOCHS):
    start = time.time()
    total_loss = 0

    # -enumerate: Combine traversable data into an index sequence
    # -batch: related to index
    for (batch, (img_tensor, target)) in enumerate(dataset):
        batch_loss, t_loss = train_step(img_tensor, target)
        total_loss += t_loss

        if batch % 100 == 0:
            print ('Epoch {} Batch {} Loss {:.4f}'.format(
              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))
    # storing the epoch end loss value to plot later
    loss_plot.append(total_loss / num_steps)

    if epoch % 5 == 0:
      ckpt_manager.save()

    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss/num_steps))
    print ('Time taken for 1 epoch {} sec\n'.format(time.time() - start))

plt.plot(loss_plot)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Plot')
plt.show()

# View checkpoint (The colab folder is not displayed, you need to manually query and download.)
filePath = './checkpoints/train/'
os.listdir(filePath)

# Select the latest checkpoints, after saving, you can directly read their weights and then perform evaluation

ckpt_manager.latest_checkpoint

with zipfile.ZipFile('ckpt-6-data.zip', 'w') as z:
    z.write('./checkpoints/train/ckpt-4.data-00000-of-00001')

with zipfile.ZipFile('ckpt-6-index.zip', 'w') as z:
    z.write('./checkpoints/train/ckpt-4.index')

import zipfile
import os

# zipfilename is the name of the compressed package, dirname is the directory to be packaged
def compress_file(zipfilename, dirname): 
    if os.path.isfile(dirname):
        with zipfile.ZipFile(zipfilename, 'w') as z:
            z.write(dirname)
    else:
        with zipfile.ZipFile(zipfilename, 'w') as z:
            for root, dirs, files in os.walk(dirname):
                for single_file in files:
                    if single_file != zipfilename:
                        filepath = os.path.join(root, single_file)
                        z.write(filepath)

compress_file('checkpoints.zip', './checkpoints/')

compress_file('encoder_data', '/content/encoder_weights.data-00000-of-00001')

"""## [10]. Captioning"""

# Encapsulation: Pay attention to loading related items 
def evaluate(image):

    # attention_plot = np.zeros((max_length, attention_features_shape))

    hidden = decoder.reset_state(batch_size=1) # reset state of hidden layer at each evaluation

    # process the image with load_(including decoding, tensor, resize, normalize), and then upgrade
    temp_input = tf.expand_dims(load_image(image)[0], 0) 
    img_tensor_val = image_features_extract_model(temp_input)  # use the interception model to export features
    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))  # dimension processing

    features = encoder(img_tensor_val)  # use custom CNN for encoding

    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0) # pre input of decoder
    result = []

    for i in range(max_length):
        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)
        # predictions is the possible output obtained after the decoder processes the image (it is a probability distribution)
        # attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()

        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy() # get the id of the word with the highest probability
        result.append(tokenizer.index_word[predicted_id]) # get word by id

        if tokenizer.index_word[predicted_id] == '<end>': # when <end> is encountered, returns
            return result 

        dec_input = tf.expand_dims([predicted_id], 0) 
        # print(dec_input.shape())

    return result

# Compare the caption of the picture itself with the predicted caption
# captions on the validation set
rid = np.random.randint(0, len(img_name_val))
image = img_name_val[rid]
real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])
result = evaluate(image)

print ('Real Caption:', real_caption)
print ('Prediction Caption:', ' '.join(result))
# plot_attention(image, result, attention_plot)

print(image)

"""## Below are not for AI developing

#### [For client developing and testing]
"""

image_path

from skimage import io
# img_src = 'https://pic4.zhimg.com/80/v2-ffeacfe2e1dd1be5d38dcc7e8ec0fb99_1440w.jpg'
img_src = 'https://image.freepik.com/free-photo/close-up-young-handsome-man-isolated_273609-35942.jpg'
image = io.imread(img_src)
io.imsave('./temp.jpg',image)
image = './temp.jpg'
# io.imshow(image)
# io.show()

# image_url = 'https://pic4.zhimg.com/80/v2-ffeacfe2e1dd1be5d38dcc7e8ec0fb99_1440w.jpg'
image_url = img_src
image_extension = image_url[-4:]
image_path = tf.keras.utils.get_file('image'+image_extension, origin=image_url)
image_path = image 
# result, attention_plot = evaluate(image_path)
result = evaluate(image_path)
print ('Prediction Caption:', ' '.join(result))
# plot_attention(image_path, result, attention_plot)
# opening the image
# Image.open(image_path)

result

encoder_1 = joblib.load('obj_encoder.pkl')
decoder_1 = joblib.load('obj_decoder.pkl')

encoder_1

encoder

img_src = 'https://pic4.zhimg.com/80/v2-ffeacfe2e1dd1be5d38dcc7e8ec0fb99_1440w.jpg'
# img_src = 'https://image.freepik.com/free-photo/close-up-young-handsome-man-isolated_273609-35942.jpg'
image = io.imread(img_src)
io.imsave('./temp.jpg',image)
image = './temp.jpg'
# io.imshow(image)
# io.show()

# image_url = 'https://pic4.zhimg.com/80/v2-ffeacfe2e1dd1be5d38dcc7e8ec0fb99_1440w.jpg'
image_url = img_src
image_extension = image_url[-4:]
image_path = tf.keras.utils.get_file('image'+image_extension, origin=image_url)
image_path = image 
# result, attention_plot = evaluate(image_path)
result = evaluate(image_path)
print ('Prediction Caption:', ' '.join(result))
# plot_attention(image_path, result, attention_plot)
# opening the image
# Image.open(image_path)

# load ckpt

# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))
ckpt.restore(ckpt_manager.latest_checkpoint)

image

decoder.save_weights('decoder_weights')
encoder.save_weights('encoder_weights')

encoder_2 = CNN_Encoder(embedding_dim)
decoder_2 = RNN_Decoder(embedding_dim, units, vocab_size)

encoder_2.load_weights('encoder_weights')
decoder_2.load_weights('decoder_weights')

# Encapsulation: Pay attention to loading related items
def evaluate_2(image):
    # attention_plot = np.zeros((max_length, attention_features_shape))

    hidden = decoder_2.reset_state(batch_size=1)  # must reset the hidden state of the hidden layer each time

    temp_input = tf.expand_dims(load_image(image)[0], 0)  # process the image with load_(including decoding, tensor, resize, normalize), then                                                                                                                                                                                升维
    img_tensor_val = image_features_extract_model(temp_input)  # use Interception-v3 model extract image features                          
    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))  # dimension processing

    features = encoder_2(img_tensor_val)  # use custom CNN for encoding

    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)  # input of decoder
    result = []

    for i in range(max_length):
        predictions, hidden, attention_weights = decoder_2(dec_input, features, hidden)
        # predictions is the possible output obtained after the decoder processes the image (it is a probability distribution)
        # attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()

        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()  # get the word id with the highest probability
        result.append(tokenizer.index_word[predicted_id])  # find word by id

        if tokenizer.index_word[predicted_id] == '<end>':  # when the end character is encountered, return
            return result

        dec_input = tf.expand_dims([predicted_id], 0)
        # print(dec_input.shape())

    return result

# image_url = 'https://pic4.zhimg.com/80/v2-ffeacfe2e1dd1be5d38dcc7e8ec0fb99_1440w.jpg' # 把猫显示为bus
image_url = 'https://image.freepik.com/free-photo/close-up-young-handsome-man-isolated_273609-35942.jpg'
image_extension = image_url[-4:]
image_path = tf.keras.utils.get_file('image'+image_extension, origin=image_url)
image_path = image 
# result, attention_plot = evaluate(image_path)
result = evaluate_2(image_path)
print ('Prediction Caption:', ' '.join(result))
# plot_attention(image_path, result, attention_plot)
# opening the image
# Image.open(image_path)

joblib.dump(tokenizer, 'tokenizer.pkl')

"""#### Check the images and the predicted captions"""

from skimage import io

# img_src = 'https://pic4.zhimg.com/80/v2-ffeacfe2e1dd1be5d38dcc7e8ec0fb99_1440w.jpg'
# img_src = 'https://image.freepik.com/free-photo/close-up-young-handsome-man-isolated_273609-35942.jpg'
# img_src = 'https://seopic.699pic.com/photo/50110/0938.jpg_wh1200.jpg'
# img_src = 'https://seopic.699pic.com/photo/50110/0947.jpg_wh1200.jpg'
# img_src = 'https://img95.699pic.com/xsj/0x/5y/o4.jpg'
img_src = 'https://seopic.699pic.com/photo/50089/1339.jpg_wh1200.jpg'
image = io.imread(img_src)
io.imsave('./temp.jpg',image)
image = './temp.jpg'
# io.imshow(image)
# io.show()

# image_url = 'https://pic4.zhimg.com/80/v2-ffeacfe2e1dd1be5d38dcc7e8ec0fb99_1440w.jpg'
image_url = img_src
image_extension = image_url[-4:]
image_path = tf.keras.utils.get_file('image'+image_extension, origin=image_url)
image_path = image 
# result, attention_plot = evaluate(image_path)
result = evaluate(image_path)
print ('Prediction Caption:', ' '.join(result))
# plot_attention(image_path, result, attention_plot)
# opening the image
Image.open(image_path)

# img_src = 'https://pic4.zhimg.com/80/v2-ffeacfe2e1dd1be5d38dcc7e8ec0fb99_1440w.jpg'
# img_src = 'https://image.freepik.com/free-photo/close-up-young-handsome-man-isolated_273609-35942.jpg'
# img_src = 'https://seopic.699pic.com/photo/50110/0938.jpg_wh1200.jpg'
# img_src = 'https://seopic.699pic.com/photo/50110/0947.jpg_wh1200.jpg'
# img_src = 'https://img95.699pic.com/xsj/0x/5y/o4.jpg'
img_src = 'https://seopic.699pic.com/photo/50102/4293.jpg_wh1200.jpg'
image = io.imread(img_src)
io.imsave('./temp.jpg',image)
image = './temp.jpg'
# io.imshow(image)
# io.show()

# image_url = 'https://pic4.zhimg.com/80/v2-ffeacfe2e1dd1be5d38dcc7e8ec0fb99_1440w.jpg'
image_url = img_src
image_extension = image_url[-4:]
image_path = tf.keras.utils.get_file('image'+image_extension, origin=image_url)
image_path = image 
# result, attention_plot = evaluate(image_path)
result = evaluate(image_path)
print ('Prediction Caption:', ' '.join(result))
# plot_attention(image_path, result, attention_plot)
# opening the image
Image.open(image_path)

# img_src = 'https://pic4.zhimg.com/80/v2-ffeacfe2e1dd1be5d38dcc7e8ec0fb99_1440w.jpg'
# img_src = 'https://image.freepik.com/free-photo/close-up-young-handsome-man-isolated_273609-35942.jpg'
# img_src = 'https://seopic.699pic.com/photo/50110/0938.jpg_wh1200.jpg'
# img_src = 'https://seopic.699pic.com/photo/50110/0947.jpg_wh1200.jpg'
# img_src = 'https://img95.699pic.com/xsj/0x/5y/o4.jpg'
img_src = 'https://seopic.699pic.com/photo/50160/6052.jpg_wh1200.jpg'
image = io.imread(img_src)
io.imsave('./temp.jpg',image)
image = './temp.jpg'
# io.imshow(image)
# io.show()

# image_url = 'https://pic4.zhimg.com/80/v2-ffeacfe2e1dd1be5d38dcc7e8ec0fb99_1440w.jpg'
image_url = img_src
image_extension = image_url[-4:]
image_path = tf.keras.utils.get_file('image'+image_extension, origin=image_url)
image_path = image 
# result, attention_plot = evaluate(image_path)
result = evaluate(image_path)
print ('Prediction Caption:', ' '.join(result))
# plot_attention(image_path, result, attention_plot)
# opening the image
Image.open(image_path)

# img_src = 'https://pic4.zhimg.com/80/v2-ffeacfe2e1dd1be5d38dcc7e8ec0fb99_1440w.jpg'
# img_src = 'https://image.freepik.com/free-photo/close-up-young-handsome-man-isolated_273609-35942.jpg'
# img_src = 'https://seopic.699pic.com/photo/50110/0938.jpg_wh1200.jpg'
# img_src = 'https://seopic.699pic.com/photo/50110/0947.jpg_wh1200.jpg'
# img_src = 'https://img95.699pic.com/xsj/0x/5y/o4.jpg'
img_src = 'https://seopic.699pic.com/photo/50089/1808.jpg_wh1200.jpg'
image = io.imread(img_src)
io.imsave('./temp.jpg',image)
image = './temp.jpg'
# io.imshow(image)
# io.show()

# image_url = 'https://pic4.zhimg.com/80/v2-ffeacfe2e1dd1be5d38dcc7e8ec0fb99_1440w.jpg'
image_url = img_src
image_extension = image_url[-4:]
image_path = tf.keras.utils.get_file('image'+image_extension, origin=image_url)
image_path = image 
# result, attention_plot = evaluate(image_path)
result = evaluate(image_path)
print ('Prediction Caption:', ' '.join(result))
# plot_attention(image_path, result, attention_plot)
# opening the image
Image.open(image_path)

# img_src = 'https://pic4.zhimg.com/80/v2-ffeacfe2e1dd1be5d38dcc7e8ec0fb99_1440w.jpg'
# img_src = 'https://image.freepik.com/free-photo/close-up-young-handsome-man-isolated_273609-35942.jpg'
# img_src = 'https://seopic.699pic.com/photo/50110/0938.jpg_wh1200.jpg'
# img_src = 'https://seopic.699pic.com/photo/50110/0947.jpg_wh1200.jpg'
# img_src = 'https://img95.699pic.com/xsj/0x/5y/o4.jpg'
img_src = 'https://photo.16pic.com/00/59/45/16pic_5945606_b.jpg'
image = io.imread(img_src)
io.imsave('./temp.jpg',image)
image = './temp.jpg' 
image_url = img_src
image_extension = image_url[-4:]
image_path = tf.keras.utils.get_file('image'+image_extension, origin=image_url)
image_path = image 
# result, attention_plot = evaluate(image_path)
result = evaluate(image_path)
print ('Prediction Caption:', ' '.join(result))
# plot_attention(image_path, result, attention_plot)
# opening the image
Image.open(image_path)

# img_src = 'https://pic4.zhimg.com/80/v2-ffeacfe2e1dd1be5d38dcc7e8ec0fb99_1440w.jpg'
# img_src = 'https://image.freepik.com/free-photo/close-up-young-handsome-man-isolated_273609-35942.jpg'
# img_src = 'https://seopic.699pic.com/photo/50110/0938.jpg_wh1200.jpg'
# img_src = 'https://seopic.699pic.com/photo/50110/0947.jpg_wh1200.jpg'
# img_src = 'https://img95.699pic.com/xsj/0x/5y/o4.jpg'
img_src = 'https://photo.16pic.com/00/59/45/16pic_5945606_b.jpg'
image = io.imread(img_src)
io.imsave('./temp.jpg',image)
image = './temp.jpg' 
image_url = img_src
image_extension = image_url[-4:]
image_path = tf.keras.utils.get_file('image'+image_extension, origin=image_url)
image_path = image 
# result, attention_plot = evaluate(image_path)
result = evaluate(image_path)
print ('Prediction Caption:', ' '.join(result))
# plot_attention(image_path, result, attention_plot)
# opening the image
Image.open(image_path)

